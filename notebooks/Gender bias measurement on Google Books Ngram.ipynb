{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15339c12",
   "metadata": {},
   "source": [
    "Credits for this code to:\n",
    "\n",
    "- Garg et. al (2018) https://github.com/nikhgarg/EmbeddingDynamicStereotypes\n",
    "- Hamilton et. al (2016) https://github.com/williamleif/histwords\n",
    "\n",
    "Credit for pretrained vectors to:\n",
    "- Zi Yin https://github.com/ziyin-dl/ngram-word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1202326a",
   "metadata": {},
   "source": [
    "## 0. Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6db83e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b2fcde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adcd14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "815bf0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b37b0e4",
   "metadata": {},
   "source": [
    "## 1. Load files and setting params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "721756be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word embeddings using gensim\n",
    "def load_KeyedVectors(path):\n",
    "    tmp_file = get_tmpfile(\"tmp_word2vec_file.txt\")\n",
    "    _ = glove2word2vec(path, tmp_file)\n",
    "    embed = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff2ca1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading word vectors for prototyping\n",
    "# Pretrained word vectors are obtained from: https://github.com/ziyin-dl/ngram-word2vec\n",
    "vec_1900 = load_KeyedVectors('../data/pretrained_vectors/vectors-1900-ngram.txt')\n",
    "vec_1950 = load_KeyedVectors('../data/pretrained_vectors/vectors-1950-ngram.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c47aba",
   "metadata": {},
   "source": [
    "### Defining target and attribute words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7da5e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_words = ['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son', 'father', 'uncle', 'grandfather']\n",
    "female_words = ['female', 'woman', 'girl', 'sister', 'she', 'her', 'daughter', 'mother', 'aunt', 'grandmother']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72547c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_names = ['John', 'Paul', 'Mike', 'Kevin', 'Steve', 'Greg', 'Jeff', 'Bill']\n",
    "female_names = ['Amy', 'Joan', 'Lisa', 'Sarah', 'Diana', 'Kate', 'Ann', 'Donna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea1a2873",
   "metadata": {},
   "outputs": [],
   "source": [
    "career = ['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career']\n",
    "family = ['home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60469421",
   "metadata": {},
   "outputs": [],
   "source": [
    "maths = ['math', 'algebra', 'geometry', 'calculus', 'equations', 'computation', 'numbers', 'addition']\n",
    "arts = ['poetry', 'art', 'Shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1e94e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "science = ['science', 'technology', 'physics', 'chemistry', 'Einstein', 'NASA', 'experiment', 'astronomy']\n",
    "arts = ['poetry', 'art', 'Shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9af95126",
   "metadata": {},
   "outputs": [],
   "source": [
    "intelligence = ['precocious', 'resourceful', 'inquisitive', 'genius', 'inventive', 'astute', 'adaptable', 'reflective',\n",
    "'discerning', 'intuitive', 'inquiring', 'judicious', 'analytical', 'apt', 'venerable', 'imaginative',\n",
    "'shrewd', 'thoughtful', 'wise', 'smart', 'ingenious', 'clever', 'brilliant', 'logical', 'intelligent']\n",
    "appearance = ['alluring', 'voluptuous', 'blushing', 'homely', 'plump', 'sensual', 'gorgeous', 'slim', 'bald',\n",
    "'athletic', 'fashionable', 'stout', 'ugly', 'muscular', 'slender', 'feeble', 'handsome', 'healthy',\n",
    "'attractive', 'fat', 'weak', 'thin', 'pretty', 'beautiful', 'strong']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b58b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "strength = ['power', 'strong', 'confident', 'dominant', 'potent', 'command', 'assert', 'loud', 'bold', 'succeed',\n",
    "'triumph', 'leader', 'shout', 'dynamic', 'winner']\n",
    "weakness = ['weak', 'surrender', 'timid', 'vulnerable', 'weakness', 'wispy', 'withdraw', 'yield', 'failure', 'shy',\n",
    "'follow', 'lose', 'fragile', 'afraid', 'loser']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ff4ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "professions = ['technician', 'accountant', 'supervisor', 'engineer', 'worker', 'educator', 'clerk', 'counselor', \n",
    "               'inspector', 'mechanic', 'manager', 'therapist', 'administrator', 'salesperson', 'receptionist', \n",
    "               'librarian',  'advisor', 'pharmacist', 'janitor', 'psychologist', 'physician', 'carpenter', 'nurse', \n",
    "               'investigator', 'bartender', 'specialist', 'electrician', 'officer', 'pathologist', 'teacher', \n",
    "               'lawyer', 'planner', 'practitioner', 'plumber', 'instructor', 'surgeon', 'veterinarian', 'paramedic', \n",
    "               'examiner', 'chemist', 'machinist', 'appraiser', 'nutritionist', 'architect', 'hairdresser', \n",
    "               'baker', 'programmer', 'paralegal', 'hygienist', 'scientist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee184b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "professions2 = [\n",
    "'janitor',\n",
    "'statistician',\n",
    "'midwife',\n",
    "'bailiff',\n",
    "'auctioneer',\n",
    "'photographer',\n",
    "'geologist',\n",
    "'shoemaker',\n",
    "'athlete',\n",
    "'cashier',\n",
    "'dancer',\n",
    "'housekeeper',\n",
    "'accountant',\n",
    "'physicist',\n",
    "'gardener',\n",
    "'dentist',\n",
    "'weaver',\n",
    "'blacksmith',\n",
    "'psychologist',\n",
    "'supervisor',\n",
    "'mathematician',\n",
    "'surveyor',\n",
    "'tailor',\n",
    "'designer',\n",
    "'economist',\n",
    "'mechanic',\n",
    "'laborer',\n",
    "'postmaster',\n",
    "'broker',\n",
    "'chemist',\n",
    "'librarian',\n",
    "'attendant',\n",
    "'clerical',\n",
    "'musician',\n",
    "'porter',\n",
    "'scientist',\n",
    "'carpenter',\n",
    "'sailor',\n",
    "'instructor',\n",
    "'sheriff',\n",
    "'pilot',\n",
    "'inspector',\n",
    "'mason',\n",
    "'baker',\n",
    "'administrator',\n",
    "'architect',\n",
    "'collector',\n",
    "'operator',\n",
    "'surgeon',\n",
    "'driver',\n",
    "'painter',\n",
    "'conductor',\n",
    "'nurse',\n",
    "'cook',\n",
    "'engineer',\n",
    "'retired',\n",
    "'sales',\n",
    "'lawyer',\n",
    "'clergy',\n",
    "'physician',\n",
    "'farmer',\n",
    "'clerk',\n",
    "'manager',\n",
    "'guard',\n",
    "'artist',\n",
    "'smith',\n",
    "'official',\n",
    "'police',\n",
    "'doctor',\n",
    "'professor',\n",
    "'student',\n",
    "'judge',\n",
    "'teacher',\n",
    "'author',\n",
    "'secretary',\n",
    "'soldier']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019b1c1a",
   "metadata": {},
   "source": [
    "## 2. Compute distance between word groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d37dffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_attr_words(vectors, word_list1, word_list2):\n",
    "    words_to_average1 = list()\n",
    "    words_to_average2 = list()\n",
    "    \n",
    "    for word1 in word_list1:\n",
    "        try:\n",
    "            words_to_average1.append(vectors[word1])\n",
    "        except:\n",
    "            pass\n",
    "            #print(\"Word is not present: \", word1)\n",
    "            \n",
    "    for word2 in word_list2:\n",
    "        try:\n",
    "            words_to_average2.append(vectors[word2])\n",
    "        except:\n",
    "            pass\n",
    "            #print(\"Word is not present: \", word2)\n",
    "            \n",
    "    #print(\"The length of average word list 1: \", len(words_to_average1))\n",
    "    #print(\"The length of average word list 2: \", len(words_to_average2))\n",
    "    \n",
    "    averaged_words1 = np.array(words_to_average1).mean(axis=0)\n",
    "    averaged_words2 = np.array(words_to_average2).mean(axis=0)\n",
    "\n",
    "    \n",
    "    return averaged_words1, averaged_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "310b54fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(v1, v2, signed = True):\n",
    "    c = np.dot(v1, v2)/np.linalg.norm(v1)/np.linalg.norm(v2)\n",
    "    if not signed:\n",
    "        return abs(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a2f5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance_between_vectors(vec1, vec2, distype = 'norm'):\n",
    "    if distype == 'norm':\n",
    "        return np.linalg.norm(np.subtract(vec1, vec2))\n",
    "    else:\n",
    "        return cossim(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea43e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_relative_norm_distance(vectors, male_word_list, female_word_list, neutral_words):\n",
    "    \n",
    "    male_avg_vec, female_avg_vec = average_attr_words(vectors, male_word_list, female_word_list)\n",
    "    \n",
    "    list_rel_norm_dist = []\n",
    "    for word in neutral_words:\n",
    "        try:\n",
    "            rel_norm_dist = calc_distance_between_vectors(vectors[word], male_avg_vec) - \\\n",
    "                            calc_distance_between_vectors(vectors[word], female_avg_vec)\n",
    "            list_rel_norm_dist.append(rel_norm_dist)\n",
    "        except:\n",
    "            pass\n",
    "            # print(\"Word is not present: \", word)\n",
    "    return np.array(list_rel_norm_dist).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26fe50e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.16624594"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the functions on career words\n",
    "calc_relative_norm_distance(vec_1950, male_words, female_words, career)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e023840",
   "metadata": {},
   "source": [
    "## 3. Calculate gender bias for all pretrained vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7264e2d",
   "metadata": {},
   "source": [
    "### For 1900s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d3daf7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all attributes word groups\n",
    "list_of_word_groups = [career, family, maths, arts, science, intelligence, appearance, strength, \n",
    "                       weakness, professions, professions2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be1c17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataframe to save the results \n",
    "computed_biases = pd.DataFrame(['career', 'family', 'maths', 'arts', 'science', 'intelligence', \n",
    "              'appearance', 'strength', 'weakness', 'professions', 'professions2'], columns=['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22d5d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/pretrained_vectors/'\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b8e0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist_in_directory(path, male_word_list, female_word_list, list_of_word_groups):\n",
    "    \n",
    "    computed_biases = pd.DataFrame(['career', 'family', 'maths', 'arts', 'science', 'intelligence', \n",
    "              'appearance', 'strength', 'weakness', 'professions', 'professions2'], columns=['group'])\n",
    "    \n",
    "    for file in files:\n",
    "        print('Analyzing file: ', file)\n",
    "        \n",
    "        file_path = path + file\n",
    "        word2vec_dict = load_KeyedVectors(file_path)\n",
    "        \n",
    "        year = re.findall(r'\\d+', file)[0]\n",
    "        \n",
    "        results = []\n",
    "        for word_group in list_of_word_groups:\n",
    "            # print('\\t Analyzing word_group: ', word_group)\n",
    "            try:\n",
    "                measure = calc_relative_norm_distance(word2vec_dict, male_word_list, female_word_list, word_group)\n",
    "                results.append(measure)\n",
    "            except:\n",
    "                results.append('NA')\n",
    "            \n",
    "        results = pd.DataFrame(results, columns=[year])\n",
    "        computed_biases = pd.concat([computed_biases, results], axis=1)\n",
    "        \n",
    "        del word2vec_dict\n",
    "        gc.collect()\n",
    "\n",
    "    return computed_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f607f7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing file:  vectors-1900-ngram.txt\n",
      "Analyzing file:  vectors-1950-ngram.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1900</th>\n",
       "      <th>1950</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.219967</td>\n",
       "      <td>-0.166246</td>\n",
       "      <td>career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.023953</td>\n",
       "      <td>0.062684</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.082020</td>\n",
       "      <td>-0.132850</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.054835</td>\n",
       "      <td>-0.124688</td>\n",
       "      <td>arts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.162713</td>\n",
       "      <td>-0.265859</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.142821</td>\n",
       "      <td>-0.161966</td>\n",
       "      <td>intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.074004</td>\n",
       "      <td>-0.057292</td>\n",
       "      <td>appearance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.175630</td>\n",
       "      <td>-0.192250</td>\n",
       "      <td>strength</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.120140</td>\n",
       "      <td>-0.127678</td>\n",
       "      <td>weakness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.097474</td>\n",
       "      <td>-0.133727</td>\n",
       "      <td>professions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.127255</td>\n",
       "      <td>-0.148257</td>\n",
       "      <td>professions2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1900      1950         group\n",
       "0  -0.219967 -0.166246        career\n",
       "1   0.023953  0.062684        family\n",
       "2  -0.082020 -0.132850         maths\n",
       "3  -0.054835 -0.124688          arts\n",
       "4  -0.162713 -0.265859       science\n",
       "5  -0.142821 -0.161966  intelligence\n",
       "6  -0.074004 -0.057292    appearance\n",
       "7  -0.175630 -0.192250      strength\n",
       "8  -0.120140 -0.127678      weakness\n",
       "9  -0.097474 -0.133727   professions\n",
       "10 -0.127255 -0.148257  professions2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the analysis, takes a while to complete\n",
    "computed_biases = calc_dist_in_directory(path, male_words, female_words, list_of_word_groups)\n",
    "computed_biases = computed_biases.reindex(sorted(computed_biases.columns), axis=1)\n",
    "computed_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff8d313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the results\n",
    "computed_biases.to_excel('gender_bias_1900s_pretrained.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27239f0b",
   "metadata": {},
   "source": [
    "## 4. Align the word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e3189",
   "metadata": {},
   "source": [
    "##  4.1 Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "224632a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_align_gensim(m1, m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.index_to_key)\n",
    "    vocab_m2 = set(m2.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1 & vocab_m2\n",
    "    if words: common_vocab &= set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1 - common_vocab and not vocab_m2 - common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.get_vecattr(w, \"count\") + m2.get_vecattr(w, \"count\"), reverse=True)\n",
    "    # print(len(common_vocab))\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1, m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.key_to_index[w] for w in common_vocab]\n",
    "        old_arr = m.vectors\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.vectors = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        new_key_to_index = {}\n",
    "        new_index_to_key = []\n",
    "        for new_index, key in enumerate(common_vocab):\n",
    "            new_key_to_index[key] = new_index\n",
    "            new_index_to_key.append(key)\n",
    "        m.key_to_index = new_key_to_index\n",
    "        m.index_to_key = new_index_to_key\n",
    "        \n",
    "        print(len(m.key_to_index), len(m.vectors))\n",
    "        \n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6df10bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25208 25208\n",
      "25208 25208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<gensim.models.keyedvectors.KeyedVectors at 0x7ff2721fda30>,\n",
       " <gensim.models.keyedvectors.KeyedVectors at 0x7ff27218b3a0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the intersection functioon\n",
    "intersection_align_gensim(vec_1900, vec_1950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5766e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"\n",
    "    Original script: https://gist.github.com/quadrismegistus/09a93e219a6ffc4f216fb85235535faf\n",
    "    Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "        \n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "\n",
    "    # patch by Richard So [https://twitter.com/richardjeanso) (thanks!) to update this code for new version of gensim\n",
    "    # base_embed.init_sims(replace=True)\n",
    "    # other_embed.init_sims(replace=True)\n",
    "\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "    \n",
    "    # re-filling the normed vectors\n",
    "    in_base_embed.fill_norms(force=True)\n",
    "    in_other_embed.fill_norms(force=True)\n",
    "    \n",
    "    # get the (normalized) embedding matrices\n",
    "    base_vecs = in_base_embed.get_normed_vectors()\n",
    "    other_vecs = in_other_embed.get_normed_vectors()\n",
    "    \n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one, i.e. multiplying the embedding matrix by \"ortho\"\n",
    "    other_embed.vectors = (other_embed.vectors).dot(ortho)    \n",
    "    \n",
    "    return other_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "729d9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the alingment function\n",
    "vec_1900_aligned = smart_procrustes_align_gensim(vec_1950, vec_1900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7597b",
   "metadata": {},
   "source": [
    "##  4.2 Align all word vectors to base vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a3f0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/pretrained_vectors/'\n",
    "files = os.listdir(path)\n",
    "embeds_1900s = [num for num in range(1900, 1902, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "042eb9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_1900s = [1900, 1950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8ce05aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading year:  1900\n",
      "Aligning year:  1900\n",
      "Writing year:  1900\n",
      "Loading year:  1950\n",
      "Aligning year:  1950\n",
      "25208 25208\n",
      "25208 25208\n",
      "Writing year:  1950\n"
     ]
    }
   ],
   "source": [
    "IN_DIR = '../data/pretrained_vectors/'\n",
    "OUT_DIR = '../data/aligned_vectors/'\n",
    "\n",
    "first_iter = True\n",
    "base_embed = None\n",
    "\n",
    "for year in embeds_1900s:\n",
    "    print(\"Loading year: \", year)\n",
    "    year_embed = load_KeyedVectors(IN_DIR + f'vectors-{year}-ngram.txt')\n",
    "    \n",
    "    print(\"Aligning year: \", year)\n",
    "    if first_iter:\n",
    "        aligned_embed = year_embed\n",
    "        first_iter = False\n",
    "    else:\n",
    "        aligned_embed = smart_procrustes_align_gensim(base_embed, year_embed)\n",
    "    base_embed = aligned_embed\n",
    "    \n",
    "    print(\"Writing year: \", year)\n",
    "    aligned_embed.save_word2vec_format(OUT_DIR + f'vectors-{year}-ngram-aligned.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd1091",
   "metadata": {},
   "source": [
    "## 5. Calculate gender bias for all aligned vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5718f0bc",
   "metadata": {},
   "source": [
    "### For 1900s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1ae2e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/aligned_vectors/'\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b95ed3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_word_groups = [career, family, maths, arts, science, intelligence, appearance, strength, \n",
    "                       weakness, professions, professions2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1312b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_biases = pd.DataFrame(['career', 'family', 'maths', 'arts', 'science', 'intelligence', \n",
    "              'appearance', 'strength', 'weakness', 'professions', 'professions2'], columns=['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ca41714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist_in_directory_aligned(path, male_word_list, female_word_list, list_of_word_groups):\n",
    "    \n",
    "    computed_biases = pd.DataFrame(['career', 'family', 'maths', 'arts', 'science', 'intelligence', \n",
    "              'appearance', 'strength', 'weakness', 'professions', 'professions2'], columns=['group'])\n",
    "    \n",
    "    for file in files:\n",
    "        print('Analyzing file: ', file)\n",
    "        \n",
    "        file_path = path + file\n",
    "        word2vec_dict = KeyedVectors.load_word2vec_format(file_path)\n",
    "        \n",
    "        year = re.findall(r'\\d+', file)[0]\n",
    "        \n",
    "        results = []\n",
    "        for word_group in list_of_word_groups:\n",
    "            # print('\\t Analyzing word_group: ', word_group)\n",
    "            try:\n",
    "                measure = calc_relative_norm_distance(word2vec_dict, male_word_list, female_word_list, word_group)\n",
    "                results.append(measure)\n",
    "            except:\n",
    "                results.append('NA')\n",
    "            \n",
    "        results = pd.DataFrame(results, columns=[year])\n",
    "        computed_biases = pd.concat([computed_biases, results], axis=1)\n",
    "        \n",
    "        del word2vec_dict\n",
    "        gc.collect()\n",
    "\n",
    "    return computed_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6f2dd0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing file:  vectors-1950-ngram-aligned.txt\n",
      "Analyzing file:  vectors-1900-ngram-aligned.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1900</th>\n",
       "      <th>1950</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.219967</td>\n",
       "      <td>-0.166246</td>\n",
       "      <td>career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.023953</td>\n",
       "      <td>0.062684</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.082020</td>\n",
       "      <td>-0.140707</td>\n",
       "      <td>maths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.054835</td>\n",
       "      <td>-0.124688</td>\n",
       "      <td>arts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.162713</td>\n",
       "      <td>-0.265859</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.142821</td>\n",
       "      <td>-0.168008</td>\n",
       "      <td>intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.074004</td>\n",
       "      <td>-0.057292</td>\n",
       "      <td>appearance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.175630</td>\n",
       "      <td>-0.191223</td>\n",
       "      <td>strength</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.120140</td>\n",
       "      <td>-0.127678</td>\n",
       "      <td>weakness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.097474</td>\n",
       "      <td>-0.124493</td>\n",
       "      <td>professions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.127255</td>\n",
       "      <td>-0.142633</td>\n",
       "      <td>professions2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1900      1950         group\n",
       "0  -0.219967 -0.166246        career\n",
       "1   0.023953  0.062684        family\n",
       "2  -0.082020 -0.140707         maths\n",
       "3  -0.054835 -0.124688          arts\n",
       "4  -0.162713 -0.265859       science\n",
       "5  -0.142821 -0.168008  intelligence\n",
       "6  -0.074004 -0.057292    appearance\n",
       "7  -0.175630 -0.191223      strength\n",
       "8  -0.120140 -0.127678      weakness\n",
       "9  -0.097474 -0.124493   professions\n",
       "10 -0.127255 -0.142633  professions2"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed_biases = calc_dist_in_directory_aligned(path, male_words, female_words, list_of_word_groups)\n",
    "computed_biases = computed_biases.reindex(sorted(computed_biases.columns), axis=1)\n",
    "computed_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ecc88850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the results\n",
    "computed_biases.to_excel('gender_bias_1900s_aligned.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
