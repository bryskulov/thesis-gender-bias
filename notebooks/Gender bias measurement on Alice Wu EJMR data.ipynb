{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e4db57",
   "metadata": {},
   "source": [
    "Credits for this code to:\n",
    "- Pierre Megret https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook\n",
    "\n",
    "The code is mainly based on his tutorial. <br>\n",
    "The gender bias measurements are done as before. <br>\n",
    "WEAT tests are performeds using wefe package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91ef77",
   "metadata": {},
   "source": [
    "# 0. Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fc7b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # For preprocessing\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd  # For data handling\n",
    "import spacy  # For preprocessing\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "import wefe\n",
    "from wefe.query import Query\n",
    "from wefe.utils import load_test_model\n",
    "from wefe.metrics import WEAT\n",
    "from wefe.word_embedding_model import WordEmbeddingModel\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52dfe1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(444810, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>raw_post</th>\n",
       "      <th>fem_all</th>\n",
       "      <th>male_all</th>\n",
       "      <th>training</th>\n",
       "      <th>ypred</th>\n",
       "      <th>female</th>\n",
       "      <th>fem_pronoun</th>\n",
       "      <th>male_pronoun</th>\n",
       "      <th>training_pronoun</th>\n",
       "      <th>ypred_pronoun</th>\n",
       "      <th>female_pronoun</th>\n",
       "      <th>time_latest</th>\n",
       "      <th>month_latest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>request-a-thread-to-be-deleted-here</td>\n",
       "      <td>The fires went out, and blank darkness fell. T...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.860989e-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2 hours</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>request-a-thread-to-be-deleted-here</td>\n",
       "      <td>Mod here. The meaning of life is  max V(A(t))=...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.082022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.067608e-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2 hours</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>request-a-thread-to-be-deleted-here</td>\n",
       "      <td>The facts established in the recent paper by W...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.032163</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.387353e-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 hours</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>request-a-thread-to-be-deleted-here</td>\n",
       "      <td>OBLANCHARD-assuming you are Olivier Blanchard ...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.526144e-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 hours</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>about-ejmr</td>\n",
       "      <td>Q: Why should I visit this site? A: There is a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.388843e-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6 days</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   title_id  post_id                                topic  \\\n",
       "0         1        4  request-a-thread-to-be-deleted-here   \n",
       "1         1        5  request-a-thread-to-be-deleted-here   \n",
       "2         1        9  request-a-thread-to-be-deleted-here   \n",
       "3         1       11  request-a-thread-to-be-deleted-here   \n",
       "4         2        2                           about-ejmr   \n",
       "\n",
       "                                            raw_post  fem_all  male_all  \\\n",
       "0  The fires went out, and blank darkness fell. T...        0         7   \n",
       "1  Mod here. The meaning of life is  max V(A(t))=...        0         1   \n",
       "2  The facts established in the recent paper by W...        0         1   \n",
       "3  OBLANCHARD-assuming you are Olivier Blanchard ...        5         5   \n",
       "4  Q: Why should I visit this site? A: There is a...        1         0   \n",
       "\n",
       "   training     ypred  female  fem_pronoun  male_pronoun  training_pronoun  \\\n",
       "0       0.0  0.001674       0            0             7               0.0   \n",
       "1       1.0  0.082022       0            0             1               1.0   \n",
       "2       1.0  0.032163       0            0             0               NaN   \n",
       "3       NaN  0.000792       0            0             0               NaN   \n",
       "4       1.0  0.000008       1            0             0               NaN   \n",
       "\n",
       "   ypred_pronoun  female_pronoun time_latest  month_latest  \n",
       "0   1.860989e-04             0.0     2 hours           0.0  \n",
       "1   8.067608e-03             0.0     2 hours           0.0  \n",
       "2   2.387353e-02             NaN     2 hours           0.0  \n",
       "3   2.526144e-02             NaN     2 hours           0.0  \n",
       "4   2.388843e-12             NaN      6 days           0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/alicewu_posts/gendered_posts.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb8db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototyping on subset of the posts\n",
    "subset = df.iloc[0:1000]\n",
    "merged_text = subset['raw_post'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c18ddd",
   "metadata": {},
   "source": [
    "# 1. Preprocess the posts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46bceced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the language\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53fb6a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a spacy instance\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e6acb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to sentenize the posts: 1.71 mins\n"
     ]
    }
   ],
   "source": [
    "# Break the posts into seperate sentences\n",
    "\n",
    "t = time()\n",
    "\n",
    "doc = nlp(merged_text)\n",
    "sents = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "print('Time to sentenize the posts: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5972631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all posts in the dataset\n",
    "\n",
    "t = time()\n",
    "\n",
    "lower_index = 0\n",
    "sents_store = list()\n",
    "\n",
    "for upper_index in range(0, 440000, 10000):\n",
    "    print(\"Processing row #: \", upper_index)\n",
    "    subset = df.iloc[lower_index:upper_index]\n",
    "    merged_text = subset['raw_post'].str.cat(sep=' ')\n",
    "    \n",
    "    t = time()\n",
    "    doc = nlp(merged_text)\n",
    "    sents = [sent.text.strip() for sent in doc.sents]\n",
    "    print('Time to sentenize the book: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "    \n",
    "    sents_store.append(sents)\n",
    "    lower_index = upper_index\n",
    "    \n",
    "print('Time to process the posts: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaa5009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the sentences\n",
    "from functools import reduce\n",
    "sents = reduce(lambda xs, ys: xs + ys, sents_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab757b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed posts\n",
    "import pickle\n",
    "with open(\"forum_posts.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(sents, fp)\n",
    "    \n",
    "# with open(\"test\", \"rb\") as fp:   # Unpickling\n",
    "#     b = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fe2d898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"forum_posts.pkl\", \"rb\") as fp:   # Unpickling\n",
    "    sents = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e3aa01",
   "metadata": {},
   "source": [
    "# 2. Clean the posts with Spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7635efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n"
     ]
    }
   ],
   "source": [
    "#loading the english language small model of spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1995a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing pronouns from spacy stopwords\n",
    "nlp.Defaults.stop_words -= {'he', 'his', 'him', 'himself', 'male', 'man', 'men'}\n",
    "nlp.Defaults.stop_words -= {'she', 'her', 'herself', 'female', 'woman', 'women'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b494972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "624316c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes non-alphabetic characters:\n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(sent)).lower() for sent in sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6166bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 0.43 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_process=-1)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c40a3164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing Nones and duplicates\n",
    "txt = list(set(filter(None, txt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a89d27",
   "metadata": {},
   "source": [
    "# 3. Identify bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c338468",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [sent.split() for sent in txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e6e43dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:32:23: collecting all words and their counts\n",
      "INFO - 16:32:23: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 16:32:23: PROGRESS: at sentence #10000, processed 88807 words and 79478 word types\n",
      "INFO - 16:32:23: PROGRESS: at sentence #20000, processed 175393 words and 140815 word types\n",
      "INFO - 16:32:23: collected 156027 token types (unigram + bigrams) from a corpus of 198307 words and 22629 sentences\n",
      "INFO - 16:32:23: merged Phrases<156027 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 16:32:23: Phrases lifecycle event {'msg': 'built Phrases<156027 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.26s', 'datetime': '2022-06-21T16:32:23.601847', 'gensim': '4.2.0', 'python': '3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \\n[Clang 6.0 (clang-600.0.57)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "610a885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:32:30: exporting phrases from Phrases<156027 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 16:32:31: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<47 phrases, min_count=30, threshold=10.0> from Phrases<156027 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.31s', 'datetime': '2022-06-21T16:32:31.296351', 'gensim': '4.2.0', 'python': '3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \\n[Clang 6.0 (clang-600.0.57)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d7993a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well',\n",
       " 'he',\n",
       " 'try',\n",
       " 's',\n",
       " 'tty',\n",
       " 'break',\n",
       " 'catalan',\n",
       " 'activist',\n",
       " 'destroy',\n",
       " 's',\n",
       " 'colonial',\n",
       " 'statue',\n",
       " 'spain']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92720a7",
   "metadata": {},
   "source": [
    "## 3.2 Most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "387d8b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19184"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "473a9543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'his', 'she', 'like', 'good', 'guy', 'people', 'think', 'paper', 'work']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "edebaf07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1453"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq['she']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422055cf",
   "metadata": {},
   "source": [
    "# 4. Train the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4464f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "998d48f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:33:39: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2022-06-21T16:33:39.079174', 'gensim': '4.2.0', 'python': '3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \\n[Clang 6.0 (clang-600.0.57)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=10,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=25,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9f7414c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:33:40: collecting all words and their counts\n",
      "INFO - 16:33:40: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 16:33:40: PROGRESS: at sentence #10000, processed 87741 words, keeping 12878 word types\n",
      "INFO - 16:33:40: PROGRESS: at sentence #20000, processed 173226 words, keeping 18101 word types\n",
      "INFO - 16:33:40: collected 19184 word types from a corpus of 195844 raw words and 22629 sentences\n",
      "INFO - 16:33:40: Creating a fresh vocabulary\n",
      "INFO - 16:33:40: Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 3019 unique words (15.74% of original 19184, drops 16165)', 'datetime': '2022-06-21T16:33:40.834658', 'gensim': '4.2.0', 'python': '3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \\n[Clang 6.0 (clang-600.0.57)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 16:33:40: Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 160825 word corpus (82.12% of original 195844, drops 35019)', 'datetime': '2022-06-21T16:33:40.835661', 'gensim': '4.2.0', 'python': '3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \\n[Clang 6.0 (clang-600.0.57)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 16:33:40: deleting the raw counts dictionary of 19184 items\n",
      "INFO - 16:33:40: sample=6e-05 downsamples 1414 most-common words\n",
      "INFO - 16:33:40: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 76965.27227663816 word corpus (47.9%% of prior 160825)', 'datetime': '2022-06-21T16:33:40.860617', 'gensim': '4.2.0', 'python': '3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \\n[Clang 6.0 (clang-600.0.57)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 16:33:40: estimated required memory for 3019 words and 300 dimensions: 8755100 bytes\n",
      "INFO - 16:33:40: resetting layer weights\n",
      "INFO - 16:33:40: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-06-21T16:33:40.913572', 'gensim': '4.2.0', 'python': '3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \\n[Clang 6.0 (clang-600.0.57)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "616bccf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 16:33:42: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 3019 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=25 window=2 shrink_windows=True', 'datetime': '2022-06-21T16:33:42.850823', 'gensim': '4.2.0', 'python': '3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \\n[Clang 6.0 (clang-600.0.57)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "INFO - 16:33:43: EPOCH 0: training on 195844 raw words (76922 effective words) took 0.3s, 239464 effective words/s\n",
      "INFO - 16:33:43: EPOCH 1: training on 195844 raw words (76999 effective words) took 0.3s, 234690 effective words/s\n",
      "INFO - 16:33:43: EPOCH 2: training on 195844 raw words (76927 effective words) took 0.3s, 262787 effective words/s\n",
      "INFO - 16:33:44: EPOCH 3: training on 195844 raw words (76861 effective words) took 0.3s, 293026 effective words/s\n",
      "INFO - 16:33:44: EPOCH 4: training on 195844 raw words (76878 effective words) took 0.3s, 252642 effective words/s\n",
      "INFO - 16:33:44: EPOCH 5: training on 195844 raw words (76970 effective words) took 0.3s, 262802 effective words/s\n",
      "INFO - 16:33:45: EPOCH 6: training on 195844 raw words (77079 effective words) took 0.4s, 195478 effective words/s\n",
      "INFO - 16:33:45: EPOCH 7: training on 195844 raw words (77153 effective words) took 0.3s, 268512 effective words/s\n",
      "INFO - 16:33:45: EPOCH 8: training on 195844 raw words (76569 effective words) took 0.3s, 261023 effective words/s\n",
      "INFO - 16:33:46: EPOCH 9: training on 195844 raw words (76895 effective words) took 0.3s, 281694 effective words/s\n",
      "INFO - 16:33:46: EPOCH 10: training on 195844 raw words (76793 effective words) took 0.3s, 256745 effective words/s\n",
      "INFO - 16:33:46: EPOCH 11: training on 195844 raw words (76984 effective words) took 0.3s, 252786 effective words/s\n",
      "INFO - 16:33:47: EPOCH 12: training on 195844 raw words (76867 effective words) took 0.4s, 196827 effective words/s\n",
      "INFO - 16:33:47: EPOCH 13: training on 195844 raw words (76931 effective words) took 0.3s, 258053 effective words/s\n",
      "INFO - 16:33:47: EPOCH 14: training on 195844 raw words (77114 effective words) took 0.3s, 279109 effective words/s\n",
      "INFO - 16:33:47: EPOCH 15: training on 195844 raw words (77081 effective words) took 0.3s, 265762 effective words/s\n",
      "INFO - 16:33:48: EPOCH 16: training on 195844 raw words (77119 effective words) took 0.3s, 254749 effective words/s\n",
      "INFO - 16:33:48: EPOCH 17: training on 195844 raw words (77029 effective words) took 0.3s, 277581 effective words/s\n",
      "INFO - 16:33:48: EPOCH 18: training on 195844 raw words (76774 effective words) took 0.3s, 281671 effective words/s\n",
      "INFO - 16:33:49: EPOCH 19: training on 195844 raw words (76939 effective words) took 0.4s, 186445 effective words/s\n",
      "INFO - 16:33:49: EPOCH 20: training on 195844 raw words (77011 effective words) took 0.3s, 265309 effective words/s\n",
      "INFO - 16:33:49: EPOCH 21: training on 195844 raw words (76909 effective words) took 0.3s, 260316 effective words/s\n",
      "INFO - 16:33:50: EPOCH 22: training on 195844 raw words (77006 effective words) took 0.3s, 283835 effective words/s\n",
      "INFO - 16:33:50: EPOCH 23: training on 195844 raw words (77085 effective words) took 0.3s, 273668 effective words/s\n",
      "INFO - 16:33:50: EPOCH 24: training on 195844 raw words (76784 effective words) took 0.3s, 257063 effective words/s\n",
      "INFO - 16:33:51: EPOCH 25: training on 195844 raw words (77052 effective words) took 0.4s, 187725 effective words/s\n",
      "INFO - 16:33:51: EPOCH 26: training on 195844 raw words (77049 effective words) took 0.3s, 261245 effective words/s\n",
      "INFO - 16:33:51: EPOCH 27: training on 195844 raw words (76991 effective words) took 0.3s, 262871 effective words/s\n",
      "INFO - 16:33:52: EPOCH 28: training on 195844 raw words (76674 effective words) took 0.3s, 235258 effective words/s\n",
      "INFO - 16:33:52: EPOCH 29: training on 195844 raw words (76767 effective words) took 0.3s, 249188 effective words/s\n",
      "INFO - 16:33:52: Word2Vec lifecycle event {'msg': 'training on 5875320 raw words (2308212 effective words) took 9.6s, 240721 effective words/s', 'datetime': '2022-06-21T16:33:52.441572', 'gensim': '4.2.0', 'python': '3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \\n[Clang 6.0 (clang-600.0.57)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.16 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e080552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6m/7qj3x7j16pqbzfycpphbz9z40000gn/T/ipykernel_8825/514372312.py:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n",
      "WARNING - 16:33:53: destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "178ede57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.9613426923751831),\n",
       " ('treat', 0.9539192914962769),\n",
       " ('bad', 0.951930820941925),\n",
       " ('male', 0.9479985237121582),\n",
       " ('sexism', 0.9462493658065796),\n",
       " ('person', 0.9418339729309082),\n",
       " ('brilliant', 0.9389874935150146),\n",
       " ('genius', 0.937942385673523),\n",
       " ('badly', 0.9377884864807129),\n",
       " ('have', 0.9351139068603516)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"woman\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed755db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('white', 0.975311279296875),\n",
       " ('hair', 0.971253514289856),\n",
       " ('ugly', 0.9683753848075867),\n",
       " ('marry', 0.9664183259010315),\n",
       " ('gay', 0.9629653692245483),\n",
       " ('beautiful', 0.9613771438598633),\n",
       " ('woman', 0.9613426923751831),\n",
       " ('attack', 0.9609489440917969),\n",
       " ('year_old', 0.9591811895370483),\n",
       " ('eye', 0.9574292898178101)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e666bcf",
   "metadata": {},
   "source": [
    "# 5. Measuring Gender Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad2bf6",
   "metadata": {},
   "source": [
    "## 5.1 Defining target and attribute words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "0a94b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_pronouns = ['her', 'herself', 'she']\n",
    "female_names = ['amy', 'athey', 'duflo', 'elizabeth', 'emily', 'hilary',\n",
    "                'hillary', 'jane', 'jennifer', 'jessica', 'maria', 'mary',\n",
    "                'nancy', 'reinhart', 'sarah', 'susan', 'yellen']\n",
    "female_identity = ['female', 'females', 'ladies', 'lady', 'woman', 'women']\n",
    "female_miscellaneous = ['bietches', 'bitch', 'bitches', 'broette', 'broettes', 'chick', 'chicks',\n",
    "                'daughter', 'daughters', 'gf', 'girl', 'girlfriend', 'girlfriends',\n",
    "                'girls', 'mom', 'moms', 'mother', 'mothers', 'sister', 'sisters', 'wife',\n",
    "                'wives']\n",
    "\n",
    "female_words = female_pronouns + female_names + female_identity + female_miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "0e76eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_pronouns = ['he', 'him', 'himself', 'his']\n",
    "male_names = ['acemoglu', 'adam', 'akerlof', 'alan', 'albert', 'alexander', 'allen', 'andrew', 'angrist', 'arthur',\n",
    "'autor', 'baker', 'barro', 'becker', 'ben', 'benjamin', 'bernanke', 'berry', 'blanchard', 'bob', 'borjas']\n",
    "male_identity = ['male', 'males', 'man', 'men']\n",
    "male_miscellaneous = ['bf', 'boy', 'boyfriend', 'boys', 'bro', 'bros', 'brother', 'brothers', 'dad', 'daddy',\n",
    "'dude', 'dudes', 'father', 'fathers', 'gentleman', 'grandfather', 'guy', 'guys',\n",
    "'husband', 'husbands', 'papa', 'sir', 'son', 'sons', 'uncle']\n",
    "\n",
    "male_words = male_pronouns + male_names + male_identity + male_miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "409700e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_names_full = ['acemoglu', 'adam', 'akerlof', 'alan', 'albert', 'alexander', 'allen', 'andrew', 'angrist', 'arthur',\n",
    "'autor', 'baker', 'barro', 'becker', 'ben', 'benjamin', 'bernanke', 'berry', 'blanchard', 'bob', 'borjas',\n",
    "'brian', 'campbell', 'carl', 'carlos', 'charlie', 'chetty', 'chris', 'christopher', 'cochrane', 'colin',\n",
    "'cowen', 'daniel', 'daron', 'david', 'deaton', 'delong', 'duffie', 'edward', 'eric', 'eugene', 'fama',\n",
    "'frank', 'frey', 'friedman', 'friedman', 'gary', 'george', 'gintis', 'glaeser', 'gordon', 'greene',\n",
    "'greg', 'gregory', 'hansen', 'harry', 'hayashi', 'hayek', 'heckman', 'henderson', 'henry', 'imben',\n",
    "'jack', 'james', 'jason', 'je', 'jerey', 'jeremy', 'jimmy', 'joe', 'john', 'jon', 'jonathan', 'jose',\n",
    "'joseph', 'justin', 'kehoe', 'ken', 'kenneth', 'kevin', 'krueger', 'kruggles', 'krugman', 'krugmans',\n",
    "'larry', 'lars', 'levine', 'levitt', 'lucas', 'mankiw', 'mark', 'martin', 'matt', 'matthew', 'michael',\n",
    "'mike', 'miller', 'milton', 'murphy', 'myerson', 'neumann', 'nicholas', 'nick', 'noah', 'parag', 'pat',\n",
    "'pathak', 'patrick', 'paul', 'perez', 'peter', 'phil', 'philip', 'phillips', 'pierre', 'piketty', 'pikettys',\n",
    "'powell', 'prescott', 'rabin', 'raj', 'ravikumar', 'ricardo', 'richard', 'robert', 'roberts', 'robinson',\n",
    "'roger', 'rogo', 'ron', 'roth', 'rubin', 'rubinstein', 'russ', 'rust', 'ryan', 'saez', 'sam', 'sargent', 'shapiro',\n",
    "'shiller', 'shleifer', 'simon', 'sims', 'stephen', 'steve', 'steven', 'stiglitz', 'summers', 'terry', 'thaler',\n",
    "'thomas', 'tim', 'tirole', 'tom', 'tony', 'victor', 'walker', 'wallace', 'walter', 'werning', 'williamson',\n",
    "'wolfer', 'woodford', 'wooldridge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "7da5e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Gender Classifiers\n",
    "male_words = ['he', 'his', 'him', 'himself', 'male', 'man', 'men']\n",
    "female_words = ['she', 'her', 'herself', 'female', 'woman', 'women']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "72547c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Academic/Professional\n",
    "economics = ['economics', 'macro', 'empirical','qje', 'keynesian']\n",
    "academic_general = ['research', 'papers','tenure', 'teaching', 'professor']\n",
    "professional = ['career', 'interview', 'payrolls', 'placement', 'recruit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "ea1a2873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personal/Physical\n",
    "personal_information = ['family','married', 'kids', 'relationship','lifestyle']\n",
    "physical_attributes = ['beautiful', 'handsome', 'attractive', 'body','fat']\n",
    "gender_related = ['gender', 'feminine', 'masculine', 'sexist', 'sexual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "60469421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swear Words\n",
    "swear = ['shit', 'wtf', 'asshole']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "d1e94e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intellectual\n",
    "intellectual_positive = ['intelligent', 'creative','competent']\n",
    "intellectual_neutral = ['brain', 'iq', 'ability']\n",
    "intellectual_negative = ['dumb','ignorant','incompetent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "9af95126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellaneous\n",
    "emotion_feelings = ['happy','depressing']\n",
    "others = ['years', 'places', 'everything']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3c8113",
   "metadata": {},
   "source": [
    "## 5.2 Defining functions for gender bias measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "d37dffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_attr_words(vectors, word_list1, word_list2):\n",
    "    words_to_average1 = list()\n",
    "    words_to_average2 = list()\n",
    "    \n",
    "    for word1 in word_list1:\n",
    "        try:\n",
    "            words_to_average1.append(vectors[word1])\n",
    "        except:\n",
    "            print(\"Word is not present: \", word1)\n",
    "            \n",
    "    for word2 in word_list2:\n",
    "        try:\n",
    "            words_to_average2.append(vectors[word2])\n",
    "        except:\n",
    "            print(\"Word is not present: \", word2)\n",
    "            \n",
    "    #print(\"The length of average word list 1: \", len(words_to_average1))\n",
    "    #print(\"The length of average word list 2: \", len(words_to_average2))\n",
    "    \n",
    "    averaged_words1 = np.array(words_to_average1).mean(axis=0)\n",
    "    averaged_words2 = np.array(words_to_average2).mean(axis=0)\n",
    "\n",
    "    \n",
    "    return averaged_words1, averaged_words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "310b54fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(v1, v2, signed = True):\n",
    "    c = np.dot(v1, v2)/np.linalg.norm(v1)/np.linalg.norm(v2)\n",
    "    if not signed:\n",
    "        return abs(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "4a2f5321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:2: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/var/folders/6m/7qj3x7j16pqbzfycpphbz9z40000gn/T/ipykernel_63599/693060765.py:2: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if distype is 'norm':\n"
     ]
    }
   ],
   "source": [
    "def calc_distance_between_vectors(vec1, vec2, distype = 'norm'):\n",
    "    if distype is 'norm':\n",
    "        return np.linalg.norm(np.subtract(vec1, vec2))\n",
    "    else:\n",
    "        return cossim(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "ea43e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_relative_norm_distance(vectors, male_word_list, female_word_list, neutral_words):\n",
    "    \n",
    "    male_avg_vec, female_avg_vec = average_attr_words(vectors, male_word_list, female_word_list)\n",
    "    \n",
    "    list_rel_norm_dist = []\n",
    "    for word in neutral_words:\n",
    "        try:\n",
    "            rel_norm_dist = calc_distance_between_vectors(vectors[word], male_avg_vec) - \\\n",
    "                            calc_distance_between_vectors(vectors[word], female_avg_vec)\n",
    "            list_rel_norm_dist.append(rel_norm_dist)\n",
    "        except:\n",
    "            print(\"Word is not present: \", word)\n",
    "    return np.array(list_rel_norm_dist).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c162b73",
   "metadata": {},
   "source": [
    "# 6. Gender Bias measurement following Garg et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "2e92c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_of_words = [economics, academic_general, professional, personal_information, physical_attributes, \n",
    "                  gender_related, intellectual_positive, intellectual_neutral, intellectual_negative]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa0d9d",
   "metadata": {},
   "source": [
    "## On gender pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "3f83c0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word is not present:  him\n",
      "Word is not present:  payrolls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.035335634"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_relative_norm_distance(w2v_model.wv, male_pronouns, female_pronouns, academic_general + professional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "3cd74a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word is not present:  him\n",
      "Word is not present:  kids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16224664"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_relative_norm_distance(w2v_model.wv, male_pronouns, female_pronouns, personal_information + physical_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27562d0",
   "metadata": {},
   "source": [
    "## On gender names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "cacb602b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08508594"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_relative_norm_distance(w2v_model.wv, male_names, female_names, economics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "f9f419cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word is not present:  jerey\n",
      "Word is not present:  rogo\n",
      "Word is not present:  payrolls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.043496635"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_relative_norm_distance(w2v_model.wv, male_names_full, female_names, academic_general + professional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "b9bcfa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word is not present:  kids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05380719"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_relative_norm_distance(w2v_model.wv, male_names, female_names, personal_information + physical_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a57ffc3",
   "metadata": {},
   "source": [
    "# 7. WEAT test by Caliscan et al. (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eede844",
   "metadata": {},
   "source": [
    "## 7.1 Check baseline bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "bb3c7131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 19:05:14: loading KeyedVectors object from /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/wefe/datasets/data/test_model.kv\n",
      "INFO - 19:05:14: setting ignored attribute vectors_norm to None\n",
      "INFO - 19:05:14: KeyedVectors lifecycle event {'fname': '/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/wefe/datasets/data/test_model.kv', 'datetime': '2022-06-14T19:05:14.734398', 'gensim': '4.2.0', 'python': '3.9.10 (v3.9.10:f2f3f53782, Jan 13 2022, 17:02:14) \\n[Clang 6.0 (clang-600.0.57)]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "# load the test model\n",
    "model = load_test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "96ecfcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "family = [\"home\", \"parents\", \"children\", \"family\", \"cousin\", \"marriage\",\n",
    "         \"wedding\"]\n",
    "career = [\"executive\", \"management\", \"professional\", \"corporation\", \"salary\",\n",
    "         \"office\", \"business\", \"career\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "33ad05ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female terms and Male Terms wrt Family and Career',\n",
       " 'result': 0.44263947810811377,\n",
       " 'weat': 0.44263947810811377,\n",
       " 'effect_size': 0.4196374793469268,\n",
       " 'p_value': 0.2014798520147985}"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the query\n",
    "query = Query(\n",
    "    target_sets=[\n",
    "        [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\",\n",
    "         \"daughter\"],\n",
    "        [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"],\n",
    "    ],\n",
    "    attribute_sets=[\n",
    "        family, career\n",
    "    ],\n",
    "    target_sets_names=[\"Female terms\", \"Male Terms\"],\n",
    "    attribute_sets_names=[\"Family\", \"Career\"],\n",
    ")\n",
    "\n",
    "\n",
    "# instance the metric and run the query\n",
    "WEAT().run_query(query, model, calculate_p_value=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a09db88",
   "metadata": {},
   "source": [
    "## 7.2 Running WEAT tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "638a5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "wefe_model = WordEmbeddingModel(w2v_model.wv, 'AliceWu model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f621d1",
   "metadata": {},
   "source": [
    "### 7.2.1 Gender pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "03f09a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query\n",
    "query_pronouns = Query(\n",
    "    target_sets=[\n",
    "        female_pronouns,\n",
    "        male_pronouns,\n",
    "    ],\n",
    "    attribute_sets=[\n",
    "        academic_general + professional,\n",
    "        personal_information + physical_attributes\n",
    "    ],\n",
    "    target_sets_names=[\"Female pronouns\", \"Male pronouns\"],\n",
    "    attribute_sets_names=[\"Professional\", \"Personal\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "c3ebdf5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female pronouns and Male pronouns wrt Professional and Personal',\n",
       " 'result': -0.7228623523066441,\n",
       " 'weat': -0.7228623523066441,\n",
       " 'effect_size': -1.8917259145115586,\n",
       " 'p_value': 0.9875173370319001}"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instance the metric and run the query\n",
    "WEAT().run_query(query_pronouns, wefe_model, normalize=True, calculate_p_value=True, \n",
    "                 p_value_method=\"approximate\", p_value_iterations=10000, lost_vocabulary_threshold=0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09991176",
   "metadata": {},
   "source": [
    "### 7.2.2 Gender names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "a46fa109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query\n",
    "query_names = Query(\n",
    "    target_sets=[\n",
    "        female_names,\n",
    "        male_names2,\n",
    "    ],\n",
    "    attribute_sets=[\n",
    "        academic_general + professional,\n",
    "        personal_information + physical_attributes\n",
    "    ],\n",
    "    target_sets_names=[\"Female Names\", \"Male Names\"],\n",
    "    attribute_sets_names=[\"Professional\", \"Personal\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "37b1aeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female Names and Male Names wrt Professional and Personal',\n",
       " 'result': -1.7645896530399718,\n",
       " 'weat': -1.7645896530399718,\n",
       " 'effect_size': -1.2123690220188075,\n",
       " 'p_value': 0.9999000099990001}"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instance the metric and run the query\n",
    "WEAT().run_query(query_names, wefe_model, normalize=True, calculate_p_value=True, \n",
    "                 p_value_method=\"approximate\", p_value_iterations=10000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ce4a1",
   "metadata": {},
   "source": [
    "### 7.2.3 All gender-identifier words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "c38e7fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query\n",
    "query_words = Query(\n",
    "    target_sets=[\n",
    "        female_words,\n",
    "        male_words,\n",
    "    ],\n",
    "    attribute_sets=[\n",
    "        academic_general + professional,\n",
    "        personal_information + physical_attributes\n",
    "    ],\n",
    "    target_sets_names=[\"Female Words\", \"Male Words\"],\n",
    "    attribute_sets_names=[\"Professional\", \"Personal\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "46241e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female Words and Male Words wrt Professional and Personal',\n",
       " 'result': -0.49194356167895925,\n",
       " 'weat': -0.49194356167895925,\n",
       " 'effect_size': -0.499921185807828,\n",
       " 'p_value': 0.8083191680831917}"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instance the metric and run the query\n",
    "WEAT().run_query(query_words, wefe_model, normalize=True, calculate_p_value=True, \n",
    "                 p_value_method=\"approximate\", p_value_iterations=10000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61009258",
   "metadata": {},
   "source": [
    "## 7.3 Run WEAT test on career and family words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "37889cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the query\n",
    "query_carfam = Query(\n",
    "    target_sets=[\n",
    "        female_words,\n",
    "        male_words,\n",
    "    ],\n",
    "    attribute_sets=[\n",
    "        career,\n",
    "        family\n",
    "    ],\n",
    "    target_sets_names=[\"Female terms\", \"Male Terms\"],\n",
    "    attribute_sets_names=[\"Career\", \"Family\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "3b80a431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_name': 'Female terms and Male Terms wrt Career and Family',\n",
       " 'result': -0.39675114249500143,\n",
       " 'weat': -0.39675114249500143,\n",
       " 'effect_size': -0.7348252153789426,\n",
       " 'p_value': 0.8983101689831017}"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instance the metric and run the query\n",
    "WEAT().run_query(query_carfam, wefe_model, normalize=True, calculate_p_value=True, \n",
    "                 p_value_method=\"approximate\", p_value_iterations=10000) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
